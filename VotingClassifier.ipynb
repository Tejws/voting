{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTOKjPWUeOky",
        "outputId": "57af249f-b67e-46ca-c882-31a3e57e6c6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Voting Classifier Accuracy: 0.8284\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Import the necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 2: Load the Titanic dataset\n",
        "titanic_data = pd.read_csv('/content/Titanic-Dataset.csv')\n",
        "\n",
        "# Step 3: Preprocess the data\n",
        "# Drop unnecessary columns like 'Name', 'Ticket', 'Cabin' (you can keep or drop others based on analysis)\n",
        "titanic_data = titanic_data.drop(columns=['Name', 'Ticket', 'Cabin', 'Embarked'])\n",
        "\n",
        "# Fill missing 'Age' values with the median of the 'Age' column\n",
        "titanic_data['Age'] = titanic_data['Age'].fillna(titanic_data['Age'].median())\n",
        "\n",
        "# Convert 'Sex' to numerical values (male = 0, female = 1)\n",
        "titanic_data['Sex'] = titanic_data['Sex'].map({'male': 0, 'female': 1})\n",
        "\n",
        "# Fill missing 'Fare' values with the median of the 'Fare' column\n",
        "titanic_data['Fare'] = titanic_data['Fare'].fillna(titanic_data['Fare'].median())\n",
        "\n",
        "# Step 4: Prepare features and target variable\n",
        "X = titanic_data.drop(columns=['Survived'])\n",
        "y = titanic_data['Survived']\n",
        "\n",
        "# Step 5: Scale the features (important for models like SVM or Logistic Regression)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 6: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 7: Define the base classifiers\n",
        "log_clf = LogisticRegression(max_iter=1000)\n",
        "svm_clf = SVC(probability=True)  # Use probability=True for soft voting\n",
        "tree_clf = DecisionTreeClassifier()\n",
        "\n",
        "# Step 8: Define the Voting Classifier\n",
        "voting_clf = VotingClassifier(estimators=[('lr', log_clf), ('svc', svm_clf), ('dt', tree_clf)], voting='soft')\n",
        "\n",
        "# Step 9: Train the Voting Classifier\n",
        "voting_clf.fit(X_train, y_train)\n",
        "\n",
        "# Step 10: Evaluate the model on the test data\n",
        "y_pred = voting_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 11: Print the accuracy\n",
        "print(f\"Voting Classifier Accuracy: {accuracy:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load Titanic dataset\n",
        "titanic_data = pd.read_csv('/content/Titanic-Dataset.csv')\n",
        "\n",
        "# Step 2: Preprocessing data\n",
        "# Handle missing data in Age, Embarked, and Fare\n",
        "# Impute missing values in 'Age', 'Fare', and 'Embarked' columns\n",
        "titanic_data['Age'] = titanic_data['Age'].fillna(titanic_data['Age'].median())\n",
        "titanic_data['Fare'] = titanic_data['Fare'].fillna(titanic_data['Fare'].median())\n",
        "titanic_data['Embarked'] = titanic_data['Embarked'].fillna(titanic_data['Embarked'].mode()[0])\n",
        "\n",
        "\n",
        "# Convert categorical features to numerical (e.g., 'Embarked', 'Sex')\n",
        "titanic_data = pd.get_dummies(titanic_data, columns=['Embarked', 'Sex'], drop_first=True)\n",
        "\n",
        "# Step 3: Split the data into features (X) and target (y)\n",
        "X = titanic_data.drop(columns=['Survived', 'Name', 'Ticket', 'Cabin'])  # Drop non-useful columns\n",
        "y = titanic_data['Survived']  # Target variable\n",
        "\n",
        "# Step 4: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Feature scaling (StandardScaler for numerical values)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Step 6: Define the base models\n",
        "log_reg = LogisticRegression(max_iter=1000)  # Logistic Regression\n",
        "svm = SVC(probability=True)  # Support Vector Machine\n",
        "decision_tree = DecisionTreeClassifier()  # Decision Tree Classifier\n",
        "\n",
        "# Step 7: Define the Voting Classifier\n",
        "# Use the soft voting method, which predicts based on the probability of each class\n",
        "voting_clf = VotingClassifier(estimators=[\n",
        "    ('log_reg', log_reg),\n",
        "    ('svm', svm),\n",
        "    ('decision_tree', decision_tree)\n",
        "], voting='soft')  # 'soft' uses predicted probabilities for majority voting\n",
        "\n",
        "# Step 8: Train the Voting Classifier\n",
        "voting_clf.fit(X_train, y_train)\n",
        "\n",
        "# Step 9: Evaluate the Voting Classifier on the test data\n",
        "y_pred = voting_clf.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the Voting Classifier\n",
        "voting_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Voting Classifier Accuracy: {voting_accuracy:.4f}\")\n",
        "\n",
        "# Step 10: Compare with individual models' accuracy\n",
        "log_reg.fit(X_train, y_train)\n",
        "log_reg_accuracy = log_reg.score(X_test, y_test)\n",
        "\n",
        "svm.fit(X_train, y_train)\n",
        "svm_accuracy = svm.score(X_test, y_test)\n",
        "\n",
        "decision_tree.fit(X_train, y_train)\n",
        "decision_tree_accuracy = decision_tree.score(X_test, y_test)\n",
        "\n",
        "# Print individual model accuracies for comparison\n",
        "print(f\"Logistic Regression Accuracy: {log_reg_accuracy:.4f}\")\n",
        "print(f\"SVM Accuracy: {svm_accuracy:.4f}\")\n",
        "print(f\"Decision Tree Accuracy: {decision_tree_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJdsMAuFgoPY",
        "outputId": "994bc3d4-b2db-47f8-f0c9-f28f4ce7bbfb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Voting Classifier Accuracy: 0.7933\n",
            "Logistic Regression Accuracy: 0.8045\n",
            "SVM Accuracy: 0.8101\n",
            "Decision Tree Accuracy: 0.7486\n"
          ]
        }
      ]
    }
  ]
}